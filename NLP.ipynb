{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd097a40",
   "metadata": {
    "id": "fd097a40"
   },
   "source": [
    "# Question1\n",
    " POETRY Generation using N-grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd8420",
   "metadata": {
    "id": "bbfd8420"
   },
   "source": [
    "1 Introduction:\n",
    "In this assignment, you will use n-gram language modeling to generate some poetry using the ngrams. For the purpose of this assignment a poem will consist of three stanzas each containing four verses where each verse consists of 7—10 words. For example, following is a manually generated stanza.\n",
    "\n",
    "دل سے نکال یاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "ہوتا ہے کیوں اداس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "مایوسیوں کی قید سے خود کو نکال کر،\n",
    "\n",
    "آ جاؤ میرے پاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "\n",
    "\n",
    "آ کر کبھی تو دید سے سیراب کر مجھے،\n",
    "\n",
    "مرتی نہیں ہے پیاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "مہر و وفا خلوص و محبت گداز دل،\n",
    "\n",
    "سب کچھ ہے میرے پاس کہ زندہ ہوں میں ابھی،\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "لوٹیں گے تیرے آتے ہی پھر دن بہار کے،\n",
    "\n",
    "رہتی ہے دل میں آس کہ زندہ ہوں میں،\n",
    "\n",
    "نایا ب شاخ چشم میں کھلتے ہیں اب بھی خواب، سچ ہے ترا\n",
    "\n",
    "قیاس کہ زندہ ہوں میں ابھی\n",
    "\n",
    "The task is to print three such stanzas with an empty line in between. The generation model can be trained on the provided Poetry Corpus containing poems from Faiz, Ghalib and Iqbal.You can scrape other urdu poetry too from internet. You will train unigram and bigram models using this corpus. These models will be used to generate poetry.\n",
    "\n",
    "2 Assignment Task:\n",
    "\n",
    "The task is to generate a poem using different models. We will generate a poem verse by verse until all stanzas have been generated. The poetry generation problem can be solved using the following algorithm:\n",
    "1. Load the Poetry Corpus\n",
    "2. Tokenize the corpus in order to split it into a list of words\n",
    "3. Generate n-gram models\n",
    "4. For each of the stanzas\n",
    "– For each verse\n",
    "* Generate a random number in the range [7...10]\n",
    "* Select first word\n",
    "* Select subsequent words until end of verse\n",
    "* [bonus] If not the first verse, try to rhyme the last word with the last word of the previous verse\n",
    "* Print verse\n",
    "– Print empty line after stanza\n",
    "2.1 Implementation Challenges:\n",
    "\n",
    "Among the challenges of solving this assignment will be selecting subsequent words once we have chosen the first word of the verse. To predict the next word, what we aim to compute is the most probable next word from all the possible next words. In other words, we need to find the set of words that occur most frequently after the already selected word and choose the next word from that set. We can use a Conditional Frequency Distribution (CFD) to figure that out! A CFD tells us: given a condition, what is likelihood of each possible outcome. [bonus] Rhyming the generated verses is also a challenge. You can build your dictionary for rhyming. The Urdu sentence is written from right to left, so makes your n-gram models according to this style.\n",
    "\n",
    "2.2 Standard n-gram Models\n",
    "We can develop our model using the Conditional Frequency Distribution method. First develop a unigram model (Unigram Model), then the bigram model (Bigram Model) and then trigram model. Select the first word of each line randomly from starting words in the vocabulary and then use the bigram model to generate the next word until the verse is complete. Generate the next three lines similarly.\n",
    " Follow the same steps for the trigram model and compare the results of the two n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb709115",
   "metadata": {
    "id": "bb709115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "افکار کے نہنگوں بھی اور سہی تو کا حجابی بے\n",
      "‘ میں اس ہے تاثیر کشِ زبونی نالہ ‘ آبادی\n",
      "عثمانی ترکان کم زر اب ہیں بھی\n",
      "پیچ ہوں جاتا کو اسی کا شاہيں ،\n",
      "\n",
      "کر پیدا جنوں اے چل آیا مقام کا نگاہ\n",
      "طالب کا ایجاد ستم بیٹھا اداس کوئی ہر\n",
      "پرواز ہے گرچہ گزر سے منزلوں جسے ہے کام ہے\n",
      "وصل ذوق بے پرکاری و کیخسرو و\n",
      "\n",
      "ہے کرے حسین رہِ گردِ پایۂ ہے\n",
      "گئیں ہو کہتے سچ آگے مرے کا مژگاں کاوشہائے\n",
      "ہے اٹھتا قلم غبار بے وعدۂ ہو گدا جو بھی\n",
      "میں جناب ہماری آبرو کی خواروں بادہ\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "\n",
    "# Load the Poetry Corpora and tokenize them\n",
    "# Replace 'iqbal.txt' and 'ghalib.txt' with the paths to your corpus files\n",
    "corpus_files = ['iqbal.txt', 'ghalib.txt']\n",
    "corpora = [open(file, 'r', encoding='utf-8').read() for file in corpus_files]\n",
    "\n",
    "# Tokenize the corpora\n",
    "words = [nltk.word_tokenize(corpus) for corpus in corpora]\n",
    "\n",
    "# Create unigram and bigram models for each corpus\n",
    "unigram_models = [nltk.FreqDist(word_list) for word_list in words]\n",
    "bigram_models = [list(nltk.bigrams(word_list)) for word_list in words]\n",
    "\n",
    "# Generate Poetry\n",
    "for stanza in range(3):\n",
    "    for verse in range(4):\n",
    "        verse_length = random.randint(7, 10)\n",
    "        verse_words = []\n",
    "\n",
    "        for word_num in range(verse_length):\n",
    "            if word_num == 0:\n",
    "                # Select the first word randomly from one of the corpora\n",
    "                selected_corpus = random.choice(corpora)\n",
    "                word = random.choice(words[corpora.index(selected_corpus)])\n",
    "            else:\n",
    "                # Use the bigram model of the selected corpus to select the next word\n",
    "                prev_word = verse_words[-1]\n",
    "                selected_corpus = corpora[corpora.index(selected_corpus) % 2]  # Alternate between corpora\n",
    "                bigram_model = bigram_models[corpora.index(selected_corpus)]\n",
    "                next_words = [w2 for w1, w2 in bigram_model if w1 == prev_word]\n",
    "                if next_words:\n",
    "                    word = random.choice(next_words)\n",
    "                else:\n",
    "                    selected_corpus = random.choice(corpora)  # If there's no next word in the bigram model, switch corpus\n",
    "                    word = random.choice(words[corpora.index(selected_corpus)])\n",
    "\n",
    "            verse_words.append(word)\n",
    "\n",
    "        # Join the words and print the verse\n",
    "        print(' '.join(verse_words[::-1]))\n",
    "\n",
    "    if stanza < 2:\n",
    "        # Print an empty line between stanzas\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2981db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79601988",
   "metadata": {
    "id": "79601988"
   },
   "source": [
    "# Question2\n",
    " Classify language out of the list given below using just stop words. Remove punctuations, make it lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa286b84",
   "metadata": {
    "id": "fa286b84",
    "outputId": "eed1da3f-fabe-4ef1-ba32-dbf482163b85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b7a398",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Maryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e058619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Maryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdad25b",
   "metadata": {
    "id": "1bdad25b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "745605e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'arabic': 0,\n",
       " 'azerbaijani': 1,\n",
       " 'basque': 0,\n",
       " 'bengali': 0,\n",
       " 'catalan': 3,\n",
       " 'chinese': 0,\n",
       " 'danish': 0,\n",
       " 'dutch': 3,\n",
       " 'english': 5,\n",
       " 'finnish': 0,\n",
       " 'french': 1,\n",
       " 'german': 1,\n",
       " 'greek': 0,\n",
       " 'hebrew': 0,\n",
       " 'hinglish': 8,\n",
       " 'hungarian': 1,\n",
       " 'indonesian': 1,\n",
       " 'italian': 2,\n",
       " 'kazakh': 0,\n",
       " 'nepali': 0,\n",
       " 'norwegian': 0,\n",
       " 'portuguese': 1,\n",
       " 'romanian': 1,\n",
       " 'russian': 0,\n",
       " 'slovene': 0,\n",
       " 'spanish': 1,\n",
       " 'swedish': 0,\n",
       " 'tajik': 0,\n",
       " 'turkish': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = \"An article is qualunque member van un class of dedicated words naquele estão used with noun phrases per mark the identifiability of the referents of the noun phrases\"\n",
    "text=text.lower()\n",
    "print(text)\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "#all avalaible languages provided by nltk\n",
    "available_languages = stopwords.fileids()\n",
    "\n",
    "\n",
    "language_scores = {}\n",
    "\n",
    "# to  match the stopword in each language...\n",
    "for language in available_languages:\n",
    "    stop_words_list = set(stopwords.words(language))\n",
    "    common_stopwords = set(words) & stop_words_list\n",
    "    score = len(common_stopwords)\n",
    "    \n",
    "    language_scores[language] = score\n",
    "# print each languages score...\n",
    "language_scores\n",
    "#for language, score in language_scores.items():\n",
    " #   print(f\"Language: {language}\\tScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "266654b6",
   "metadata": {
    "id": "266654b6",
    "outputId": "38cd33e4-19c0-4338-a6af-6b2951888fe9"
   },
   "outputs": [],
   "source": [
    "# Your output looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f2698",
   "metadata": {
    "id": "d15f2698"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43708fa5",
   "metadata": {
    "id": "43708fa5"
   },
   "source": [
    "# Question 3\n",
    " Rule Based Roman Urdu Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f761e",
   "metadata": {
    "id": "2e1f761e"
   },
   "source": [
    "Roman Urdu lacks standard lexicon and usually many spelling variations exist for a given word, e.g., the word zindagi (life) is also written as zindagee, zindagy, zaindagee and zndagi. So, in this question you have to Normalize Roman Urdu words using the following Rules given in the attached Pdf. Your Code works for a complete Sentence or multiple sentences.\n",
    "\n",
    "For Example: zaroori, zaruri, zarori map to the 'zrory'. So zrory becomes the correct word for all representations mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586577da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd7e7159",
   "metadata": {
    "id": "dd7e7159"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Maryam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the word: maryam\n",
      "Normalized Sentence: maryam\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def normalize_roman_urdu_sentence(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Define custom normalization rules for start and end substrings\n",
    "    start_rules = [\n",
    "        ('ai', 'ae'), \n",
    "        ('es','is'),# Replace 'zi' at the start with 'zy'\n",
    "        # Add more start rules as needed\n",
    "    ]\n",
    "\n",
    "    end_rules = [\n",
    "        ('ain', 'ein'), \n",
    "        ('ay', 'e'),\n",
    "        ('ey', 'e'),\n",
    "        ('ie', 'y'),\n",
    "        ('ai', 'ae'),\n",
    "        ('ri','ry')\n",
    "        \n",
    "        # Replace 'ri' at the end with 'ry'\n",
    "        # Add more end rules as needed\n",
    "    ]\n",
    "    multiple_rules=[\n",
    "                    ('aa','a'),\n",
    "                    ('ee','i'),\n",
    "                    ('ss','s'),\n",
    "                    ('jj','j'),\n",
    "                    ('oo','o'),\n",
    "                    ('dd','d'),\n",
    "                   \n",
    "    ]\n",
    "    rand_rules=[\n",
    "        ('ai','ae'),\n",
    "        ('ih','eh'),\n",
    "        ('iy','i'),\n",
    "        ('u','o'),\n",
    "         \n",
    "    ]\n",
    "    except_end=[\n",
    "        ('ry','ri'),\n",
    "        ('sy','si'),\n",
    "        ('ty','ti'),\n",
    "        \n",
    "    ]\n",
    "    except_start=[\n",
    "        ('ar','r')\n",
    "    ]\n",
    "    # Apply the rules to each word in the sentence\n",
    "    normalized_words = []\n",
    "    for word in words:\n",
    "        if word[-1]=='i' :\n",
    "            word=word[:-1]+word[-1].replace(\"i\",\"y\")\n",
    "            #print(word[-1:])\n",
    "        else:\n",
    "            word=word\n",
    "        for pattern, replacement in start_rules:\n",
    "            if word.startswith(pattern):\n",
    "                word = replacement + word[len(pattern):]\n",
    "        for pattern, replacement in end_rules:\n",
    "            if word.endswith(pattern):\n",
    "                word = word[:-len(pattern)] + replacement\n",
    "        for pattern, replacement in multiple_rules:\n",
    "            while pattern in word:\n",
    "                word = word.replace(pattern, replacement)\n",
    "        for pattern, replacement in rand_rules:\n",
    "            word = word.replace(pattern, replacement)\n",
    "        for pattern, replacement in except_end:\n",
    "            if word.endswith(pattern):\n",
    "                word=word\n",
    "            else:\n",
    "                word = word.replace(pattern, replacement)\n",
    "        for pattern, replacement in except_start:\n",
    "            if word.endswith(pattern):\n",
    "                word=word\n",
    "            else:\n",
    "                word = word.replace(pattern, replacement)\n",
    "       # print(word[:-1])  \n",
    "        if \"ihh\" in word: #needs work\n",
    "            word = word.replace(\"ihh\",\"ey\")\n",
    "            while \"hh\" in word:\n",
    "                word = word.replace(\"hh\",\"\")\n",
    "                word = word.replace(\"ey\",\"eh\")\n",
    "        if \"iyy\" in word:  #needs work\n",
    "                word = word.replace(\"iy\",\"I\")\n",
    "                while \"yy\" in string:\n",
    "                    word = word.replace(\"y\",\"\")\n",
    "        normalized_words.append(word)       \n",
    "        \n",
    "    return normalized_words  # Return the list of normalized words\n",
    "\n",
    "# Test the function with a sentence\n",
    "sentence_to_normalize = input(\"Enter the word: \")\n",
    "normalized_words = normalize_roman_urdu_sentence(sentence_to_normalize)\n",
    "normalized_sentence = ' '.join(normalized_words)\n",
    "\n",
    "print(\"Normalized Sentence:\", normalized_sentence)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "406b1c91",
   "metadata": {
    "id": "m7iy18K4vJYt"
   },
   "source": [
    "# Question 4\n",
    "In this question, you have been given two text files in Urdu. The first file contains an Urdu dictionary,\n",
    "which consists of a list of words. The second file contains sentences that do not have spaces between the\n",
    "words and are difficult to read.\n",
    "آجخودبخشہوں\n",
    "This sentence, without proper word segmentation, is difficult to understand. However, with proper word\n",
    "segmentation, the sentence can be separated into individual words:\n",
    "آج خود بخش ہوں\n",
    "This makes the sentence much easier to read and understand.\n",
    "\n",
    "\n",
    "This task is create spaces between words using\n",
    "\n",
    "*   unigrams\n",
    "*   bigram\n",
    "*   trigrams\n",
    "\n",
    "You can use the list of words file/dictionary provided in assignment 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "142c871a",
   "metadata": {
    "id": "142c871a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تجربہ کارہ ندو ستانی آفس پنر روی چندر نا یش ون نے آئند ہا یش یا ء کپ 2 0 2 3 ء کی غیری قی نی قسمت پراپن ی رائے کااظہار کیاہے جوپ اکستا نمی ں ہونے جارہاہے اپنے یوٹیوب چینل پربا تکر تے ہوئے روی چندر نا یش ون نےکہا کہا گرپ ڑ وسیم لک بھارت ایشیا کپ 2 0 2 3 ء میں شرکت کرنا چاہتاہے توم\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def segment_text(text, dictionary):\n",
    "    n = len(text)\n",
    "    segmented_text = []\n",
    "    i = 0\n",
    "\n",
    "    while i < n:\n",
    "        found = None\n",
    "\n",
    "        for j in range(i + 18, i, -1):  # Consider a maximum word length of 18 characters\n",
    "            if text[i:j] in dictionary:\n",
    "                found = text[i:j]\n",
    "                break\n",
    "\n",
    "        if found:\n",
    "            segmented_text.append(found)\n",
    "            i += len(found)\n",
    "        else:\n",
    "            segmented_text.append(text[i])\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(segmented_text)\n",
    "\n",
    "# Load the Urdu dictionary from a text file into a set\n",
    "with open('words.txt', 'r', encoding='utf-8') as file:\n",
    "    urdu_dictionary = {line.strip() for line in file}\n",
    "\n",
    "# Load the Urdu text from a text file\n",
    "with open('word_test.txt', 'r', encoding='utf-8') as file:\n",
    "    urdu_text = file.read()\n",
    "\n",
    "# Segment the Urdu text using unigrams, bigrams, and trigrams\n",
    "segmented_urdu_text = segment_text(urdu_text, urdu_dictionary)\n",
    "\n",
    "# Print the segmented text\n",
    "print(segmented_urdu_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2c10e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800953c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf49ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc32afdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129eaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5379f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
